{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOMdkk8KWzhz+V2tTpeGgb6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kemadjou-Elodie/Deep-Learning-Projet/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kFHa3A38gqe"
      },
      "source": [
        "## **Multi-class Text Categorization**\r\n",
        "\r\n",
        "**Objective**. In this technical test you you are asked to solve a multi-class classification problem on\r\n",
        "textual data using machine learning, commonly known as text categorization. The technical\r\n",
        "test is composed of two parts, model experimentation and deployment. More details will be explained\r\n",
        "in the corresponding sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy23LOuG9Dkc"
      },
      "source": [
        "######  Load the data - The dataset comprises around 18000 documents posts on 20 topics split in two subsets : one for training and another for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtWOJYYUzeLu",
        "outputId": "de802851-bdda-4f69-e836-59fafa99bf60"
      },
      "source": [
        " ! pip  install  googledrivedownloader"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzMeyFG_w748"
      },
      "source": [
        "import os\r\n",
        "import nltk\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "from spacy.tokenizer import Tokenizer\r\n",
        "from sklearn.svm import LinearSVC\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.feature_extraction.text import TfidfTransformer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5mrKkArzmO5"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader  as gdd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgNGHWSHxzys",
        "outputId": "78cd2027-5499-427d-a68a-d4d56131d669"
      },
      "source": [
        "gdd.download_file_from_google_drive(file_id ='1ywHLd78-Ms5SmyEuHGmJDDGHSGsvvcD2', dest_path ='./dataset.zip', unzip=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1ywHLd78-Ms5SmyEuHGmJDDGHSGsvvcD2 into ./dataset.zip... Done.\n",
            "Unzipping...Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebKPvA-8LKI3"
      },
      "source": [
        "data_dir_test = '/content/test'\r\n",
        "data_dir_train = '/content/train'\r\n",
        "df_test = os.listdir(data_dir_test)\r\n",
        "df_train = os.listdir(data_dir_train)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMRKVAx_xqS8"
      },
      "source": [
        "from os import listdir\r\n",
        "from os.path import isfile, join\r\n",
        "files = []\r\n",
        "for folder_name in df_test:\r\n",
        "    folder_path = join(data_dir_test, folder_name)\r\n",
        "    files.append([f for f in listdir(folder_path)])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BgzVWyQyMzD",
        "outputId": "24ec457c-1f9a-48d2-c4f4-7dea198a377c"
      },
      "source": [
        "sum(len(files[i]) for i in range(20))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7532"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oug85ccmNpJy",
        "outputId": "347fee31-6846-485c-8f2d-0fdbf01cc8e9"
      },
      "source": [
        "df_test"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['soc.religion.christian',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'alt.atheism',\n",
              " 'talk.politics.mideast',\n",
              " 'rec.sport.hockey',\n",
              " 'talk.politics.misc',\n",
              " 'sci.space',\n",
              " 'comp.windows.x',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.crypt',\n",
              " 'misc.forsale',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.autos',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'talk.religion.misc',\n",
              " 'talk.politics.guns']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GetBNfBOgXd",
        "outputId": "7039a6fc-7789-4a57-c515-f70121cf6ba7"
      },
      "source": [
        "df_train"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['soc.religion.christian',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'alt.atheism',\n",
              " 'talk.politics.mideast',\n",
              " 'rec.sport.hockey',\n",
              " 'talk.politics.misc',\n",
              " 'sci.space',\n",
              " 'comp.windows.x',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.crypt',\n",
              " 'misc.forsale',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.autos',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'talk.religion.misc',\n",
              " 'talk.politics.guns']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLWJrnkEYcLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47cd14a1-1a6b-4800-9e3e-1046cb6a94a9"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\r\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4DkiZtMJfNx",
        "outputId": "342ee2d3-ee73-4e11-c9ff-7123a400fbee"
      },
      "source": [
        "from pprint import pprint\r\n",
        "pprint(list(newsgroups_train.target_names))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['alt.atheism',\n",
            " 'comp.graphics',\n",
            " 'comp.os.ms-windows.misc',\n",
            " 'comp.sys.ibm.pc.hardware',\n",
            " 'comp.sys.mac.hardware',\n",
            " 'comp.windows.x',\n",
            " 'misc.forsale',\n",
            " 'rec.autos',\n",
            " 'rec.motorcycles',\n",
            " 'rec.sport.baseball',\n",
            " 'rec.sport.hockey',\n",
            " 'sci.crypt',\n",
            " 'sci.electronics',\n",
            " 'sci.med',\n",
            " 'sci.space',\n",
            " 'soc.religion.christian',\n",
            " 'talk.politics.guns',\n",
            " 'talk.politics.mideast',\n",
            " 'talk.politics.misc',\n",
            " 'talk.religion.misc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vd_wvVyJsL0",
        "outputId": "3ec29589-2872-4ba6-8ec6-7506eba2b5be"
      },
      "source": [
        "newsgroups_train.filenames.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOIhRv2eJ2iI",
        "outputId": "40e9c182-bbee-452c-912e-befe869e8600"
      },
      "source": [
        "newsgroups_train.target.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8-Bcyl0eeuG"
      },
      "source": [
        "Pour le preprocessing je commence avec le modele back of words ou **tf-idf** pour voir a quel point tu parviens classifier les differentes categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETz6jIzzJ-Yw",
        "outputId": "481e2b20-1fd0-4af8-dba5-f271ed2a0166"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\r\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\r\n",
        "\r\n",
        "vectorizer = TfidfVectorizer()\r\n",
        "vectors = vectorizer.fit_transform(newsgroups_train.data)\r\n",
        "vectors.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2034, 34118)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vX27m6nfBgP",
        "outputId": "40d0226c-d281-43f7-ca5a-9b20a8e97bdc"
      },
      "source": [
        "vectors.nnz / float(vectors.shape[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "159.0132743362832"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVM3XdeYfFBt"
      },
      "source": [
        "Les vecteurs TF-IDF extraits sont très rares, avec une moyenne de 159 composantes non nulles par échantillon dans un espace de plus de 30000 dimensions (moins de 0,5% d'entités non nulles)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geNW_Q06L64b"
      },
      "source": [
        "#importing dataset from sklearn\r\n",
        "from sklearn.datasets import fetch_20newsgroups\r\n",
        "#importing train and test dataset\r\n",
        "train_df= fetch_20newsgroups(subset=\"train\" ,categories = categories) \r\n",
        "test_df= fetch_20newsgroups(subset=\"test\" ,categories = categories)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tSp8vr8P619"
      },
      "source": [
        "X_train = train_df[\"data\"]\r\n",
        "X_test=test_df['data']\r\n",
        "y_train = train_df[\"target\"] \r\n",
        "y_test=test_df['target']"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJJfE4z1R5tX"
      },
      "source": [
        "df=pd.DataFrame(X_train,columns=['mess'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0QdCjUOlJsW"
      },
      "source": [
        "#adding a target column\r\n",
        "df['target']=y_train"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "g8Bb0kIIlROn",
        "outputId": "adfd345c-8d12-4308-d7c3-eaf259a6a1e7"
      },
      "source": [
        "#making length a feature for visualizations\r\n",
        "df['length']=df['mess'].apply(len)\r\n",
        "df.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mess</th>\n",
              "      <th>target</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>From: rych@festival.ed.ac.uk (R Hawkes)\\nSubje...</td>\n",
              "      <td>1</td>\n",
              "      <td>1022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Subject: Re: Biblical Backing of Koresh's 3-02...</td>\n",
              "      <td>3</td>\n",
              "      <td>1117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>From: Mark.Perew@p201.f208.n103.z1.fidonet.org...</td>\n",
              "      <td>2</td>\n",
              "      <td>572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>From: dpw@sei.cmu.edu (David Wood)\\nSubject: R...</td>\n",
              "      <td>0</td>\n",
              "      <td>1454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From: prb@access.digex.com (Pat)\\nSubject: Con...</td>\n",
              "      <td>2</td>\n",
              "      <td>449</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                mess  target  length\n",
              "0  From: rych@festival.ed.ac.uk (R Hawkes)\\nSubje...       1    1022\n",
              "1  Subject: Re: Biblical Backing of Koresh's 3-02...       3    1117\n",
              "2  From: Mark.Perew@p201.f208.n103.z1.fidonet.org...       2     572\n",
              "3  From: dpw@sei.cmu.edu (David Wood)\\nSubject: R...       0    1454\n",
              "4  From: prb@access.digex.com (Pat)\\nSubject: Con...       2     449"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8vACC1ERXNG"
      },
      "source": [
        "# Text Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvZig2WRQdJ4",
        "outputId": "b98412e5-2e94-4133-ed06-ecfd84162e0c"
      },
      "source": [
        "#importing string for punctuations\r\n",
        "import string\r\n",
        "import nltk\r\n",
        "#now we import most common words i.e. stopwords\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKMw2AdgRJYq"
      },
      "source": [
        "#making a function to process our data\r\n",
        "def text_process(mess):\r\n",
        "    no_punc=[c for c in mess if c not in string.punctuation]\r\n",
        "    no_punc=''.join(no_punc)\r\n",
        "    cleaned_mess=[word for word in no_punc.split() if word.lower() not in stopwords.words('english')]\r\n",
        "    return cleaned_mess"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5la2DKuRNjU"
      },
      "source": [
        "##applying our text_process function\r\n",
        "#adding processed data to a new column\r\n",
        "df['message']=df['mess'].apply(text_process)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WawEm5tYmUY"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "lexicon = (\r\n",
        "    (re.compile(r\"\\bdon't\\b\"), \"do not\"),\r\n",
        "    (re.compile(r\"\\bit's\\b\"), \"it is\"),\r\n",
        "    (re.compile(r\"\\bi'm\\b\"), \"i am\"),\r\n",
        "    (re.compile(r\"\\bi've\\b\"), \"i have\"),\r\n",
        "    (re.compile(r\"\\bcan't\\b\"), \"cannot\"),\r\n",
        "    (re.compile(r\"\\bdoesn't\\b\"), \"does not\"),\r\n",
        "    (re.compile(r\"\\bthat's\\b\"), \"that is\"),\r\n",
        "    (re.compile(r\"\\bdidn't\\b\"), \"did not\"),\r\n",
        "    (re.compile(r\"\\bi'd\\b\"), \"i would\"),\r\n",
        "    (re.compile(r\"\\byou're\\b\"), \"you are\"),\r\n",
        "    (re.compile(r\"\\bisn't\\b\"), \"is not\"),\r\n",
        "    (re.compile(r\"\\bi'll\\b\"), \"i will\"),\r\n",
        "    (re.compile(r\"\\bthere's\\b\"), \"there is\"),\r\n",
        "    (re.compile(r\"\\bwon't\\b\"), \"will not\"),\r\n",
        "    (re.compile(r\"\\bwoudn't\\b\"), \"would not\"),\r\n",
        "    (re.compile(r\"\\bhe's\\b\"), \"he is\"),\r\n",
        "    (re.compile(r\"\\bthey're\\b\"), \"they are\"),\r\n",
        "    (re.compile(r\"\\bwe're\\b\"), \"we are\"),\r\n",
        "    (re.compile(r\"\\blet's\\b\"), \"let us\"),\r\n",
        "    (re.compile(r\"\\bhaven't\\b\"), \"have not\"),\r\n",
        "    (re.compile(r\"\\bwhat's\\b\"), \"what is\"),\r\n",
        "    (re.compile(r\"\\baren't\\b\"), \"are not\"),\r\n",
        "    (re.compile(r\"\\bwasn't\\b\"), \"was not\"),\r\n",
        "    (re.compile(r\"\\bwouldn't\\b\"), \"would not\"),\r\n",
        ")\r\n",
        "\r\n",
        "def fix_apostrophes(text):\r\n",
        "    text = text.lower()\r\n",
        "    \r\n",
        "    for pattern, replacement in lexicon:\r\n",
        "        text = pattern.sub(replacement, text)\r\n",
        "\r\n",
        "    return text\r\n",
        "\r\n",
        "text_train = list(map(fix_apostrophes, train_df))\r\n",
        "text_test = list(map(fix_apostrophes, test_df))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE8torEhUxbl"
      },
      "source": [
        "# Normalization & Vecorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1jKzg8qRVt9"
      },
      "source": [
        "#Importing CountVectorizer to a collection of text documents to a matrix of token counts.\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7Nvh-c0U3Kz",
        "outputId": "6e559f2f-3f3c-428b-b67c-134629bae02d"
      },
      "source": [
        "bow_transformer = CountVectorizer(analyzer=text_process).fit(df['message'])\r\n",
        "# Print total number of vocab words\r\n",
        "print(len(bow_transformer.vocabulary_))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i01emy_tp0sM"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C9mVmktpsqA"
      },
      "source": [
        "Pour la classification j'utiliser un algo comme les machine a support de vecteur (Support vector machine SVM) \r\n",
        "\r\n",
        "Pour l'evaluation du modele tu peux utiliser le F-beta score le macro/micro average"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h98y8o9Pri5i",
        "outputId": "f69a1fde-7df7-4b57-f588-f3e0764b6715"
      },
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer()),\r\n",
        "                     ('tfidf', TfidfTransformer()),\r\n",
        "                     ('clf', LinearSVC()),\r\n",
        "                     ])\r\n",
        "\r\n",
        "text_clf.fit(X_train, y_train)\r\n",
        "\r\n",
        "\r\n",
        "predicted = text_clf.predict(X_test)\r\n",
        "\r\n",
        "print(metrics.classification_report(y_test, predicted))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.83      0.84       319\n",
            "           1       0.92      0.97      0.94       389\n",
            "           2       0.95      0.95      0.95       394\n",
            "           3       0.81      0.76      0.79       251\n",
            "\n",
            "    accuracy                           0.89      1353\n",
            "   macro avg       0.88      0.88      0.88      1353\n",
            "weighted avg       0.89      0.89      0.89      1353\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UamsnDf6r-Qf"
      },
      "source": [
        "from sklearn.metrics import fbeta_score"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV4AACVDsdcJ",
        "outputId": "b863c99e-e46a-425e-e387-2b2deca0c437"
      },
      "source": [
        "fbeta_score(y_test, predicted, average='macro', beta=0.5)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8823503546816361"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p05bHlWdsi32"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMnyglFwwDK4iDu+qr3UZch",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kemadjou-Elodie/Deep-Learning-Projet/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtWOJYYUzeLu",
        "outputId": "7ab63325-b093-42b4-d953-ced6e367d5ed"
      },
      "source": [
        " ! pip  install  googledrivedownloader"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzMeyFG_w748"
      },
      "source": [
        "import nltk\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "from spacy.tokenizer import Tokenizer\r\n",
        "from sklearn.svm import LinearSVC\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.feature_extraction.text import TfidfTransformer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5mrKkArzmO5"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader  as gdd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgNGHWSHxzys",
        "outputId": "85555957-ccd6-4e82-c154-bd8fc7ef7a70"
      },
      "source": [
        "gdd.download_file_from_google_drive(file_id ='1ywHLd78-Ms5SmyEuHGmJDDGHSGsvvcD2', dest_path ='./dataset.zip', unzip=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1ywHLd78-Ms5SmyEuHGmJDDGHSGsvvcD2 into ./dataset.zip... Done.\n",
            "Unzipping...Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLWJrnkEYcLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eadc8944-8676-41e7-c4d2-62c5863005a8"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\r\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4DkiZtMJfNx",
        "outputId": "be010baa-1907-4bb3-8a5d-cf29682b5096"
      },
      "source": [
        "from pprint import pprint\r\n",
        "pprint(list(newsgroups_train.target_names))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['alt.atheism',\n",
            " 'comp.graphics',\n",
            " 'comp.os.ms-windows.misc',\n",
            " 'comp.sys.ibm.pc.hardware',\n",
            " 'comp.sys.mac.hardware',\n",
            " 'comp.windows.x',\n",
            " 'misc.forsale',\n",
            " 'rec.autos',\n",
            " 'rec.motorcycles',\n",
            " 'rec.sport.baseball',\n",
            " 'rec.sport.hockey',\n",
            " 'sci.crypt',\n",
            " 'sci.electronics',\n",
            " 'sci.med',\n",
            " 'sci.space',\n",
            " 'soc.religion.christian',\n",
            " 'talk.politics.guns',\n",
            " 'talk.politics.mideast',\n",
            " 'talk.politics.misc',\n",
            " 'talk.religion.misc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vd_wvVyJsL0",
        "outputId": "61df90d8-f6c4-4895-ddd1-0a33038f9530"
      },
      "source": [
        "newsgroups_train.filenames.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOIhRv2eJ2iI",
        "outputId": "d1137103-e5d6-48c9-af44-c29af30e10c3"
      },
      "source": [
        "newsgroups_train.target.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXg9BDnvJ54I",
        "outputId": "f2562e97-996d-49b9-ca8d-5f5fc422bed9"
      },
      "source": [
        "newsgroups_train.target[:10]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8-Bcyl0eeuG"
      },
      "source": [
        "Pour le preprocessing je commence avec le modele back of words ou **tf-idf** pour voir a quel point tu parviens classifier les differentes categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETz6jIzzJ-Yw",
        "outputId": "ac419b07-c50f-4e2c-bffc-e35fd8ec4c0b"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\r\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\r\n",
        "\r\n",
        "vectorizer = TfidfVectorizer()\r\n",
        "vectors = vectorizer.fit_transform(newsgroups_train.data)\r\n",
        "vectors.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2034, 34118)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vX27m6nfBgP",
        "outputId": "6ae59108-7dc7-4e9f-d44d-14a4cef4f582"
      },
      "source": [
        "vectors.nnz / float(vectors.shape[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "159.0132743362832"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVM3XdeYfFBt"
      },
      "source": [
        "Les vecteurs TF-IDF extraits sont très rares, avec une moyenne de 159 composantes non nulles par échantillon dans un espace de plus de 30000 dimensions (moins de 0,5% d'entités non nulles)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geNW_Q06L64b"
      },
      "source": [
        "#importing dataset from sklearn\r\n",
        "from sklearn.datasets import fetch_20newsgroups\r\n",
        "#importing train and test dataset\r\n",
        "train_df= fetch_20newsgroups(subset=\"train\" ,categories = categories) \r\n",
        "test_df= fetch_20newsgroups(subset=\"test\" ,categories = categories)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tSp8vr8P619"
      },
      "source": [
        "X_train = train_df[\"data\"]\r\n",
        "X_test=test_df['data']\r\n",
        "y_train = train_df[\"target\"] \r\n",
        "y_test=test_df['target']"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJJfE4z1R5tX"
      },
      "source": [
        "df=pd.DataFrame(X_train,columns=['mess'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0QdCjUOlJsW"
      },
      "source": [
        "#adding a target column\r\n",
        "df['target']=y_train"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "g8Bb0kIIlROn",
        "outputId": "1b739615-70b0-43c2-e2bc-f11fac18ca39"
      },
      "source": [
        "#making length a feature for visualizations\r\n",
        "df['length']=df['mess'].apply(len)\r\n",
        "df.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mess</th>\n",
              "      <th>target</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>From: rych@festival.ed.ac.uk (R Hawkes)\\nSubje...</td>\n",
              "      <td>1</td>\n",
              "      <td>1022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Subject: Re: Biblical Backing of Koresh's 3-02...</td>\n",
              "      <td>3</td>\n",
              "      <td>1117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>From: Mark.Perew@p201.f208.n103.z1.fidonet.org...</td>\n",
              "      <td>2</td>\n",
              "      <td>572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>From: dpw@sei.cmu.edu (David Wood)\\nSubject: R...</td>\n",
              "      <td>0</td>\n",
              "      <td>1454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From: prb@access.digex.com (Pat)\\nSubject: Con...</td>\n",
              "      <td>2</td>\n",
              "      <td>449</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                mess  target  length\n",
              "0  From: rych@festival.ed.ac.uk (R Hawkes)\\nSubje...       1    1022\n",
              "1  Subject: Re: Biblical Backing of Koresh's 3-02...       3    1117\n",
              "2  From: Mark.Perew@p201.f208.n103.z1.fidonet.org...       2     572\n",
              "3  From: dpw@sei.cmu.edu (David Wood)\\nSubject: R...       0    1454\n",
              "4  From: prb@access.digex.com (Pat)\\nSubject: Con...       2     449"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8vACC1ERXNG"
      },
      "source": [
        "# Text Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvZig2WRQdJ4",
        "outputId": "137a29f3-cd9e-4ee1-a746-189eef2af641"
      },
      "source": [
        "#importing string for punctuations\r\n",
        "import string\r\n",
        "import nltk\r\n",
        "#now we import most common words i.e. stopwords\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKMw2AdgRJYq"
      },
      "source": [
        "#making a function to process our data\r\n",
        "def text_process(mess):\r\n",
        "    no_punc=[c for c in mess if c not in string.punctuation]\r\n",
        "    no_punc=''.join(no_punc)\r\n",
        "    cleaned_mess=[word for word in no_punc.split() if word.lower() not in stopwords.words('english')]\r\n",
        "    return cleaned_mess"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5la2DKuRNjU"
      },
      "source": [
        "##applying our text_process function\r\n",
        "#adding processed data to a new column\r\n",
        "df['message']=df['mess'].apply(text_process)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE8torEhUxbl"
      },
      "source": [
        "# Normalization & Vecorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1jKzg8qRVt9"
      },
      "source": [
        "#Importing CountVectorizer to a collection of text documents to a matrix of token counts.\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7Nvh-c0U3Kz",
        "outputId": "67a417b2-1c66-45ae-dcd9-f460eeb8c8e0"
      },
      "source": [
        "bow_transformer = CountVectorizer(analyzer=text_process).fit(df['message'])\r\n",
        "# Print total number of vocab words\r\n",
        "print(len(bow_transformer.vocabulary_))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i01emy_tp0sM"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C9mVmktpsqA"
      },
      "source": [
        "Pour la classification j'utiliser un algo comme les machine a support de vecteur (Support vector machine SVM) \r\n",
        "\r\n",
        "Pour l'evaluation du modele tu peux utiliser le F-beta score le macro/micro average"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h98y8o9Pri5i",
        "outputId": "8bdea464-2144-496a-ed0d-4576e0409649"
      },
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer()),\r\n",
        "                     ('tfidf', TfidfTransformer()),\r\n",
        "                     ('clf', LinearSVC()),\r\n",
        "                     ])\r\n",
        "\r\n",
        "text_clf.fit(X_train, y_train)\r\n",
        "\r\n",
        "\r\n",
        "predicted = text_clf.predict(X_test)\r\n",
        "\r\n",
        "print(metrics.classification_report(y_test, predicted))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.83      0.84       319\n",
            "           1       0.92      0.97      0.94       389\n",
            "           2       0.95      0.95      0.95       394\n",
            "           3       0.81      0.76      0.79       251\n",
            "\n",
            "    accuracy                           0.89      1353\n",
            "   macro avg       0.88      0.88      0.88      1353\n",
            "weighted avg       0.89      0.89      0.89      1353\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UamsnDf6r-Qf"
      },
      "source": [
        "from sklearn.metrics import fbeta_score"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV4AACVDsdcJ",
        "outputId": "05474530-8961-41a5-91d4-23ba4160c7ba"
      },
      "source": [
        "fbeta_score(y_test, predicted, average='macro', beta=0.5)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8823503546816361"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p05bHlWdsi32"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}